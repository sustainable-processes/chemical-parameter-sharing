{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "# import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Disables RDKit whiny logging.\n",
    "\"\"\"\n",
    "import rdkit.rdBase as rkrb\n",
    "import rdkit.RDLogger as rkl\n",
    "logger = rkl.logger()\n",
    "logger.setLevel(rkl.ERROR)\n",
    "rkrb.DisableLog('rdApp.error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the data preprocessed in USPTO_preprocessing.ipynb\n",
    "# There's around 500k reactions, and columns for reactant, product, solvent, reagent, etc.\n",
    "# So there's quite a bit more data than in Modelling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in pickled clean data\n",
    "cleaned_df = pd.read_pickle(f\"data/ORD_USPTO/cleaned_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the reaction classes\n",
    "rxn_classes_filename = '/Users/dsw46/nextmove/HazELNut/build/data/classified_rxn.smi'\n",
    "\n",
    "with open(rxn_classes_filename) as f:\n",
    "    lines = f.readlines()\n",
    "lines = [line.rstrip('\\n') for line in lines] # remove the \\n at the end of each line\n",
    "\n",
    "# create df of the reaction classes\n",
    "# 2 columns: mapped_rxn, rxn_classes\n",
    "rxns = []\n",
    "rxn_classes = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        rxn, rxn_class = line.split(' ')\n",
    "        rxns += [rxn]\n",
    "        rxn_classes += [rxn_class]\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    \n",
    "rxn_classes_df = pd.DataFrame(list(zip(rxns, rxn_classes)),\n",
    "               columns =['mapped_rxn', 'rxn_class'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the two dfs\n",
    "data_df_temp = cleaned_df.merge(rxn_classes_df, how='inner', left_on='mapped_rxn_0', right_on='mapped_rxn')\n",
    "len(data_df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526999\n"
     ]
    }
   ],
   "source": [
    "# I used the following command to generate the rxn classification:\n",
    "# ./namerxn -nomap data/mapped_rxn.smi data/classified_rxn.smi\n",
    "\n",
    "# The -nomap I thought would mean that it wouldn't change the atom mapping, yet it clearly did...\n",
    "# I'll just have to trust that namerxn didn't change the order of my reactions, and just append the reaction classes, and finally remove any reactions that couldn't be classified\n",
    "data_df = cleaned_df.copy().reset_index(drop=True)\n",
    "data_df['rxn_class'] = rxn_classes_df['rxn_class']\n",
    "data_df = data_df.dropna(subset=['rxn_class'])\n",
    "data_df.reset_index()\n",
    "print(len(data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419295\n"
     ]
    }
   ],
   "source": [
    "# remove all the unclassified reactions, ie where rxn_class = '0.0'\n",
    "remove_unclassified_rxn_data_df = data_df[~data_df.rxn_class.str.contains(\"0.0\")]\n",
    "print(len(remove_unclassified_rxn_data_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the cleaning that Alexander did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out all catalysts\n",
    "#sorted(list(set(df['catalyst_0'].dropna())))\n",
    "\n",
    "# initialize a dict that maps catalysts to the humanly cleaned smiles\n",
    "catalyst_replacements = {}\n",
    "\n",
    "catalyst_wrong = []\n",
    "# All the data should have already been cleaned using rdkit.canonsmiles so I'm very surprised that there are some catalysts that are wrong. If you see any wrong catalysts, just remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a catalyst to the catalyst_replacements dict\n",
    "catalyst_replacements['CC(=O)[O-].CC(=O)[O-].CC(=O)[O-].CC(=O)[O-].[Rh+3].[Rh+3]'] = 'CC(=O)[O-].CC(=O)[O-].CC(=O)[O-].CC(=O)[O-].[Rh+2].[Rh+2]'\n",
    "catalyst_replacements['[CC(=O)[O-].CC(=O)[O-].CC(=O)[O-].[Rh+3]]'] = 'CC(=O)[O-].CC(=O)[O-].CC(=O)[O-].CC(=O)[O-].[Rh+2].[Rh+2]'\n",
    "catalyst_replacements['[CC(C)(C)[P]([Pd][P](C(C)(C)C)(C(C)(C)C)C(C)(C)C)(C(C)(C)C)C(C)(C)C]'] = 'CC(C)(C)[PH]([Pd][PH](C(C)(C)C)(C(C)(C)C)C(C)(C)C)(C(C)(C)C)C(C)(C)C'\n",
    "catalyst_replacements['CCCC[N+](CCCC)(CCCC)CCCC.CCCC[N+](CCCC)(CCCC)CCCC.CCCC[N+](CCCC)(CCCC)CCCC.[Br-].[Br-].[Br-]'] = 'CCCC[N+](CCCC)(CCCC)CCCC.[Br-]'\n",
    "catalyst_replacements['[CCO.CCO.CCO.CCO.[Ti]]'] = 'CCO[Ti](OCC)(OCC)OCC'\n",
    "catalyst_replacements['[CC[O-].CC[O-].CC[O-].CC[O-].[Ti+4]]'] = 'CCO[Ti](OCC)(OCC)OCC'\n",
    "catalyst_replacements['[Cl[Ni]Cl.c1ccc(P(CCCP(c2ccccc2)c2ccccc2)c2ccccc2)cc1]'] = 'Cl[Ni]1(Cl)[P](c2ccccc2)(c2ccccc2)CCC[P]1(c1ccccc1)c1ccccc1'\n",
    "catalyst_replacements['[Cl[Pd](Cl)([P](c1ccccc1)(c1ccccc1)c1ccccc1)[P](c1ccccc1)(c1ccccc1)c1ccccc1]'] = 'Cl[Pd](Cl)([PH](c1ccccc1)(c1ccccc1)c1ccccc1)[PH](c1ccccc1)(c1ccccc1)c1ccccc1'\n",
    "catalyst_replacements['[Cl[Pd+2](Cl)(Cl)Cl.[Na+].[Na+]]'] = 'Cl[Pd]Cl'\n",
    "catalyst_replacements['Karstedt catalyst'] = 'C[Si](C)(C=C)O[Si](C)(C)C=C.[Pt]'\n",
    "catalyst_replacements[\"Karstedt's catalyst\"] = 'C[Si](C)(C=C)O[Si](C)(C)C=C.[Pt]'\n",
    "catalyst_replacements['[O=C([O-])[O-].[Ag+2]]'] = 'O=C([O-])[O-].[Ag+].[Ag+]'\n",
    "catalyst_replacements['[O=S(=O)([O-])[O-].[Ag+2]]'] = 'O=S(=O)([O-])[O-].[Ag+].[Ag+]'\n",
    "catalyst_replacements['[O=[Ag-]]'] = 'O=[Ag]'\n",
    "catalyst_replacements['[O=[Cu-]]'] = 'O=[Cu]'\n",
    "catalyst_replacements['[Pd on-carbon]'] = '[C].[Pd]'\n",
    "catalyst_replacements['[TEA]'] = 'OCCN(CCO)CCO'\n",
    "catalyst_replacements['[Ti-superoxide]'] = 'O=[O-].[Ti]'\n",
    "catalyst_replacements['[[Pd].c1ccc(P(c2ccccc2)c2ccccc2)cc1]'] = '[Pd].c1ccc(P(c2ccccc2)c2ccccc2)cc1.c1ccc(P(c2ccccc2)c2ccccc2)cc1.c1ccc(P(c2ccccc2)c2ccccc2)cc1.c1ccc(P(c2ccccc2)c2ccccc2)cc1'\n",
    "catalyst_replacements['[c1ccc([PH](c2ccccc2)(c2ccccc2)[Pd-4]([PH](c2ccccc2)(c2ccccc2)c2ccccc2)([PH](c2ccccc2)(c2ccccc2)c2ccccc2)[PH](c2ccccc2)(c2ccccc2)c2ccccc2)cc1]'] = 'c1ccc([PH](c2ccccc2)(c2ccccc2)[Pd]([PH](c2ccccc2)(c2ccccc2)c2ccccc2)([PH](c2ccccc2)(c2ccccc2)c2ccccc2)[PH](c2ccccc2)(c2ccccc2)c2ccccc2)cc1'\n",
    "catalyst_replacements['[c1ccc([P]([Pd][P](c2ccccc2)(c2ccccc2)c2ccccc2)(c2ccccc2)c2ccccc2)cc1]'] = 'c1ccc([PH](c2ccccc2)(c2ccccc2)[Pd]([PH](c2ccccc2)(c2ccccc2)c2ccccc2)([PH](c2ccccc2)(c2ccccc2)c2ccccc2)[PH](c2ccccc2)(c2ccccc2)c2ccccc2)cc1'\n",
    "catalyst_replacements['[c1ccc([P](c2ccccc2)(c2ccccc2)[Pd]([P](c2ccccc2)(c2ccccc2)c2ccccc2)([P](c2ccccc2)(c2ccccc2)c2ccccc2)[P](c2ccccc2)(c2ccccc2)c2ccccc2)cc1]'] = 'c1ccc([PH](c2ccccc2)(c2ccccc2)[Pd]([PH](c2ccccc2)(c2ccccc2)c2ccccc2)([PH](c2ccccc2)(c2ccccc2)c2ccccc2)[PH](c2ccccc2)(c2ccccc2)c2ccccc2)cc1'\n",
    "catalyst_replacements['[sulfated tin oxide]'] = 'O=S(O[Sn])(O[Sn])O[Sn]'\n",
    "catalyst_replacements['[tereakis(triphenylphosphine)palladium(0)]'] = 'c1ccc([PH](c2ccccc2)(c2ccccc2)[Pd]([PH](c2ccccc2)(c2ccccc2)c2ccccc2)([PH](c2ccccc2)(c2ccccc2)c2ccccc2)[PH](c2ccccc2)(c2ccccc2)c2ccccc2)cc1'\n",
    "catalyst_replacements['[zeolite]'] = 'O=[Al]O[Al]=O.O=[Si]=O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add any wrong catalysts you spot, e.g.\n",
    "catalyst_wrong += ['Catalyst A',\n",
    "'catalyst',\n",
    "'catalyst 1',\n",
    "'catalyst A',\n",
    "'catalyst VI',\n",
    "'reaction mixture',\n",
    "'same catalyst',\n",
    "'solution']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows that contain a 'catalyst_wrong\n",
    "df2 = data_df[~data_df[\"catalyst_0\"].isin(catalyst_wrong)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the catalyst replacements that Alexander found\n",
    "df3 = df2.replace(catalyst_replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Pd in the reagents columns:  1205\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(data_df['reagents_0'])):\n",
    "    r = data_df['reagents_0'][i]\n",
    "    if r ==r:\n",
    "        if 'pd' in r or 'Pd' in r or 'palladium' in r or 'Palladium' in r:\n",
    "            count +=1\n",
    "print('Number of Pd in the reagents columns: ', count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quite a few of the rows have Pd as a reagent. Probably worth going through all of them, and if the value in reagent_0 is already in catalyst_0, then replace the reagent value with np.NaN\n",
    "df3[\"reagents_0\"] = df3.apply(lambda x: np.nan if (pd.notna(x[\"reagents_0\"]) and pd.notna(x[\"catalyst_0\"]) and x[\"reagents_0\"] in x[\"catalyst_0\"]) else x[\"reagents_0\"], axis=1)\n",
    "df3[\"reagents_1\"] = df3.apply(lambda x: np.nan if (pd.notna(x[\"reagents_1\"]) and pd.notna(x[\"catalyst_0\"]) and x[\"reagents_1\"] in x[\"catalyst_0\"]) else x[\"reagents_1\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That took care of a majority of the cases! Now there are only 9+7 cases left, just drop these rows\n",
    "df3 = df3[df3[\"reagents_0\"] != '[Pd]']\n",
    "df3 = df3[df3[\"reagents_0\"] != '[Pd+2]']\n",
    "df3 = df3[df3[\"reagents_1\"] != '[Pd]']\n",
    "df3 = df3[df3[\"reagents_1\"] != '[Pd+2]']\n",
    "df3 = df3.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Pd in the reagents columns:  0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(df3['reagents_1'])):\n",
    "    r = df3['reagents_1'][i]\n",
    "    if r ==r:\n",
    "        if 'Pd' in r:\n",
    "            print(r)\n",
    "            count +=1\n",
    "print('Number of Pd in the reagents columns: ', count )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526802"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a cluster column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['rxn_super_class'] = df3['rxn_class'].str.rsplit('.', expand=True)[0].astype(int)\n",
    "test_df = df3['rxn_class'].str.rsplit(';', expand=True)\n",
    "# 2.5% of reactions have been assigned 2 reaction classes. 3 or 4 reaction classes is very rare."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dsw46/opt/anaconda3/envs/chemistry/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from modelling_3 import calc_fp\n",
    "from modelling_3 import calc_fp_individual\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from rdkit.rdBase import BlockLogs\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526802/526802 [00:25<00:00, 20821.09it/s]\n",
      "100%|██████████| 526802/526802 [00:46<00:00, 11251.81it/s]\n",
      "100%|██████████| 526802/526802 [00:48<00:00, 10817.74it/s]\n",
      "100%|██████████| 526802/526802 [00:48<00:00, 10831.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35 s, sys: 22.2 s, total: 57.2 s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "inputs = tqdm(df3['product_0'])\n",
    "p0 = Parallel(n_jobs=num_cores)(delayed(calc_fp_individual)(i) for i in inputs)\n",
    "\n",
    "inputs = tqdm(df3['product_1'])\n",
    "p1 = Parallel(n_jobs=num_cores)(delayed(calc_fp_individual)(i) for i in inputs)\n",
    "\n",
    "inputs = tqdm(df3['product_2'])\n",
    "p2 = Parallel(n_jobs=num_cores)(delayed(calc_fp_individual)(i) for i in inputs)\n",
    "\n",
    "inputs = tqdm(df3['product_3'])\n",
    "p3 = Parallel(n_jobs=num_cores)(delayed(calc_fp_individual)(i) for i in inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 526802/526802 [00:18<00:00, 28112.02it/s]\n",
      "100%|██████████| 526802/526802 [00:17<00:00, 29400.55it/s]\n",
      "100%|██████████| 526802/526802 [00:44<00:00, 11746.15it/s]\n",
      "100%|██████████| 526802/526802 [00:47<00:00, 11029.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 s, sys: 15.4 s, total: 43.6 s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "inputs = tqdm(df3['reactant_0'])\n",
    "r0 = Parallel(n_jobs=num_cores)(delayed(calc_fp_individual)(i) for i in inputs)\n",
    "\n",
    "inputs = tqdm(df3['reactant_1'])\n",
    "r1 = Parallel(n_jobs=num_cores)(delayed(calc_fp_individual)(i) for i in inputs)\n",
    "\n",
    "inputs = tqdm(df3['reactant_2'])\n",
    "r2 = Parallel(n_jobs=num_cores)(delayed(calc_fp_individual)(i) for i in inputs)\n",
    "\n",
    "inputs = tqdm(df3['reactant_3'])\n",
    "r3 = Parallel(n_jobs=num_cores)(delayed(calc_fp_individual)(i) for i in inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rxn difference fp\n",
    "# converting one 500k by 2k list to array takes roughly 15s, so the whole thing should take about 2-3 min\n",
    "# need to split into different cells for memory purposes\n",
    "ar_p0 = np.array(p0)\n",
    "ar_p1 = np.array(p1)\n",
    "ar_p2 = np.array(p2)\n",
    "ar_p3 = np.array(p3)\n",
    "ar_r0 = np.array(r0)\n",
    "ar_r1 = np.array(r1)\n",
    "ar_r2 = np.array(r2)\n",
    "ar_r3 = np.array(r3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(526802, 2048)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_fp = ar_p0 + ar_p1 + ar_p2 + ar_p3\n",
    "product_fp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(526802, 2048)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rxn_diff_fp = product_fp - ar_r0 - ar_r1 - ar_r2 - ar_r3\n",
    "rxn_diff_fp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to pickle\n",
    "np.save(\"data/ORD_USPTO/USPTO_rxn_diff_fp.pkl\", rxn_diff_fp)\n",
    "np.save(\"data/ORD_USPTO/USPTO_product_fp.pkl\", product_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gao model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pubs.acs.org/doi/full/10.1021/acscentsci.8b00357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpickle\n",
    "rxn_diff_fp = np.load(\"data/ORD_USPTO/USPTO_rxn_diff_fp.pkl.npy\", allow_pickle=True)\n",
    "product_fp = np.load(\"data/ORD_USPTO/USPTO_product_fp.pkl.npy\", allow_pickle=True)\n",
    "# Run all cells in the \"Read in data\" section to get data_df\n",
    "\n",
    "# find the data in df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rxn_diff_fp has the wrong shape, I'll figure out what went wrong when I get home\n",
    "# Probably just delete the files and rerun the nb\n",
    "rxn_diff_fp = rxn_diff_fp[:526802]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input for model\n",
    "np_input = np.concatenate((product_fp, rxn_diff_fp), axis=1)\n",
    "\n",
    "X = torch.from_numpy(np_input).float()\n",
    "\n",
    "# prepare output for model\n",
    "enc = OneHotEncoder()\n",
    "catalyst_encoded = enc.fit_transform(df3[['catalyst_0']])\n",
    "solvent_encoded = enc.fit_transform(df3[['solvent_0']])\n",
    "\n",
    "catalyst_encoded = catalyst_encoded.toarray()\n",
    "solvent_encoded = solvent_encoded.toarray()\n",
    "\n",
    "y_catalyst = torch.from_numpy(catalyst_encoded)\n",
    "y_solvent = torch.from_numpy(solvent_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "\n",
    "X_train, X_test, y_catalyst_train, y_catalyst_test, y_solvent_train, y_solvent_test = train_test_split(X, y_catalyst, y_solvent, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([105361, 4096])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use torch.nn\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        # Architecture\n",
    "        self.input_layer = nn.Linear(X_train.shape[1], 1000)\n",
    "        \n",
    "        self.linear1000_1000 = nn.Linear(1000, 1000)\n",
    "        self.linear1000_500 = nn.Linear(1000, 500)\n",
    "        self.linear1000_300 = nn.Linear(1000, 300)\n",
    "        self.linear600_300 = nn.Linear(600, 300)\n",
    "        self.linear500_300 = nn.Linear(500, 300)\n",
    "        self.linear300_300 = nn.Linear(300, 300)\n",
    "        \n",
    "        # Catalyst_0 output layer\n",
    "        self.catalyst_output = nn.Linear(300, len(df3['catalyst_0'].unique()))\n",
    "        \n",
    "        \n",
    "        # Solvent_0 output layer\n",
    "        self.solvent_input = nn.Linear(len(df3['catalyst_0'].unique()), 100)\n",
    "\n",
    "        self.solvent_output = nn.Linear(300, len(df3['solvent_0'].unique()))\n",
    "        \n",
    "        # Activation functions and dropout softmax (stuff with parameters)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear1000_1000(x)\n",
    "        x = self.relu(x)\n",
    "        cat_latent_1 = self.dropout(x)\n",
    "        x = self.linear1000_300(cat_latent_1)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear300_300(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        # Catalyst_0 output\n",
    "        catalyst_out = self.catalyst_output(x)\n",
    "        catalyst_out = self.softmax(catalyst_out)\n",
    "        \n",
    "        # Solvent_0 output\n",
    "        solvent_input = self.solvent_input(catalyst_out)\n",
    "        x = self.relu(solvent_input)\n",
    "        cat_latent_2 = self.linear1000_500(cat_latent_1)\n",
    "        x = torch.cat((x, cat_latent_2), 1)\n",
    "        x = self.linear600_300(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear300_300(x)\n",
    "        x = self.tanh(x)\n",
    "        solvent_out = self.solvent_output(x)\n",
    "        solvent_out = self.softmax(solvent_out)\n",
    "        \n",
    "        return catalyst_out, solvent_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet()\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the number of training iterations\n",
    "n_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 10.742929916730793\n",
      "Iteration 1, Loss: 10.739783855805914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[39m# Backward pass and update the parameters\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 41\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     44\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIteration \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chemistry/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chemistry/lib/python3.10/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chemistry/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chemistry/lib/python3.10/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/chemistry/lib/python3.10/site-packages/torch/optim/adam.py:364\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    363\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n\u001b[1;32m    367\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the batch size\n",
    "batch_size = 3200\n",
    "\n",
    "# Create data loaders for the training data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(X_train, y_catalyst_train, y_solvent_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create data loader for test data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(X_test, y_catalyst_test, y_solvent_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the number of training iterations\n",
    "n_iterations = 100\n",
    "\n",
    "# Training the model\n",
    "for i in range(n_iterations):\n",
    "    for X_batch, y_catalyst_batch, y_solvent_batch in train_loader:\n",
    "        # Forward pass\n",
    "        catalyst_out, solvent_out = model(X_batch)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss_catalyst = criterion(catalyst_out, y_catalyst_batch)\n",
    "        loss_solvent = criterion(solvent_out, y_solvent_batch)\n",
    "        loss = loss_catalyst + loss_solvent\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backward pass and update the parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if i % 1 == 0:\n",
    "        print(f'Iteration {i}, Loss: {loss.item()}')\n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dsw46/opt/anaconda3/envs/chemistry/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import trange\n",
    "\n",
    "from modelling_3 import FullyConnectedReactionModel\n",
    "from modelling_3 import train_loop\n",
    "\n",
    "e=70\n",
    "batch_size=0.07\n",
    "lr=1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpickle\n",
    "rxn_diff_fp = np.load(\"data/ORD_USPTO/USPTO_rxn_diff_fp.pkl.npy\", allow_pickle=True)\n",
    "# Run all cells in the \"Read in data\" section to get data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(526999, 2048)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rxn_diff_fp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mapped_rxn_0', 'reactant_0', 'reactant_1', 'reactant_2', 'reactant_3',\n",
       "       'reagents_0', 'reagents_1', 'solvent_0', 'solvent_1', 'catalyst_0',\n",
       "       'temperature_0', 'rxn_time_0', 'rxn_time_1', 'product_0', 'product_1',\n",
       "       'product_2', 'product_3', 'yields_0', 'yields_1', 'yields_2',\n",
       "       'yields_3', 'rxn_class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the data\n",
    "cat = np.array(data_df['catalyst_0'])\n",
    "# Do the one-hot encoding\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "cat_reshaped = cat.reshape(-1, 1)\n",
    "_ = enc.fit(cat_reshaped)\n",
    "\n",
    "cat_ohe = enc.transform(cat.reshape(-1, 1)).toarray()\n",
    "\n",
    "rxn_diff_fp_train, rxn_diff_fp_val, cat_ohe_train, cat_ohe_val = train_test_split(rxn_diff_fp, cat_ohe, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big NN (full sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 69 Train [ Loss: 0.15031 | Acc (Top 1): 0.96769 | Acc (Top 3): 0.98997 | Acc (Top 5): 0.99344]\t Validation [ Loss: 0.81797 | Acc (Top 1): 0.92578 | Acc (Top 3): 0.96625 | Acc (Top 5):  0.97294]: 100%|██████████| 70/70 [58:29<00:00, 50.14s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m scheduler \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m early_stopper \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# EarlyStopper(patience=50, min_delta=0, verbose=True)\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m hist \u001b[39m=\u001b[39m train_loop(fcrm, x_train, y_train, epochs\u001b[39m=\u001b[39;49me, batch_size\u001b[39m=\u001b[39;49mbatch_size, loss_fn\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mCrossEntropyLoss(), optimizer\u001b[39m=\u001b[39;49moptimizer, report_freq\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, x_val\u001b[39m=\u001b[39;49mx_val, y_val\u001b[39m=\u001b[39;49my_val, scheduler\u001b[39m=\u001b[39;49mscheduler, early_stopper\u001b[39m=\u001b[39;49mearly_stopper)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofCambridge/Cambridge/Projects/chem_param_sharing_joe/chemical-parameter-sharing/modelling_3.py:222\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, x_train, y_train, epochs, batch_size, loss_fn, optimizer, report_freq, scheduler, early_stopper, train_cluster_ids_for_downstream, train_similarity_dist, x_val, y_val, val_cluster_ids_for_downstream, val_similarity_dist)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m val_loss_trajectory:\n\u001b[1;32m    219\u001b[0m     val_traj \u001b[39m=\u001b[39m {\n\u001b[1;32m    220\u001b[0m         (\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m): val_loss_trajectory, (\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39macc\u001b[39m\u001b[39m\"\u001b[39m): val_acc_trajectory, (\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39macc_top3\u001b[39m\u001b[39m\"\u001b[39m): val_acc_trajectory_top3, (\u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39macc_top5\u001b[39m\u001b[39m\"\u001b[39m): val_acc_trajectory_top5\n\u001b[1;32m    221\u001b[0m     }\n\u001b[0;32m--> 222\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrain_traj, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mval_traj}, index\u001b[39m=\u001b[39mreport_epochs)\n\u001b[1;32m    223\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(train_traj, index\u001b[39m=\u001b[39mreport_epochs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# A data through one model for all reactions\n",
    "# rxn_diff_fp width -> ohe width\n",
    "\n",
    "x_train = torch.Tensor(rxn_diff_fp_train)\n",
    "y_train = torch.Tensor(cat_ohe_train)\n",
    "x_val = torch.Tensor(rxn_diff_fp_val)\n",
    "y_val = torch.Tensor(cat_ohe_val)\n",
    "fcrm = FullyConnectedReactionModel(\n",
    "    input_dim=x_train.shape[1],\n",
    "    hidden_dims=[600, 600],\n",
    "    target_dim=y_train.shape[1],\n",
    "    hidden_act=torch.nn.ReLU, \n",
    "    output_act=torch.nn.Identity, \n",
    "    use_batchnorm=True, \n",
    "    dropout_prob=0.2,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(fcrm.parameters(), lr=lr)\n",
    "scheduler = None  # torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)\n",
    "early_stopper = None  # EarlyStopper(patience=50, min_delta=0, verbose=True)\n",
    "\n",
    "hist = train_loop(fcrm, x_train, y_train, epochs=e, batch_size=batch_size, loss_fn=torch.nn.CrossEntropyLoss(), optimizer=optimizer, report_freq=1, x_val=x_val, y_val=y_val, scheduler=scheduler, early_stopper=early_stopper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m f,ax\u001b[39m=\u001b[39mplt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m15\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m hist\u001b[39m.\u001b[39mloc[:, (\u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m), \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m)]\u001b[39m.\u001b[39mdroplevel(level\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mplot(title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCross Entropy (Loss)\u001b[39m\u001b[39m\"\u001b[39m, ax\u001b[39m=\u001b[39max[\u001b[39m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m hist\u001b[39m.\u001b[39mloc[:, (\u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m), \u001b[39m\"\u001b[39m\u001b[39macc\u001b[39m\u001b[39m\"\u001b[39m)]\u001b[39m.\u001b[39mdroplevel(level\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mplot(title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAccuracy\u001b[39m\u001b[39m\"\u001b[39m, ax\u001b[39m=\u001b[39max[\u001b[39m1\u001b[39m])\n\u001b[1;32m      4\u001b[0m hist\u001b[39m.\u001b[39mloc[:, (\u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m), \u001b[39m\"\u001b[39m\u001b[39macc_top3\u001b[39m\u001b[39m\"\u001b[39m)]\u001b[39m.\u001b[39mdroplevel(level\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mplot(title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTop 3 Accuracy\u001b[39m\u001b[39m\"\u001b[39m, ax\u001b[39m=\u001b[39max[\u001b[39m2\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f,ax=plt.subplots(1,4, figsize=(15, 5))\n",
    "hist.loc[:, (slice(None), \"loss\")].droplevel(level=1,axis=1).plot(title=\"Cross Entropy (Loss)\", ax=ax[0])\n",
    "hist.loc[:, (slice(None), \"acc\")].droplevel(level=1,axis=1).plot(title=\"Accuracy\", ax=ax[1])\n",
    "hist.loc[:, (slice(None), \"acc_top3\")].droplevel(level=1,axis=1).plot(title=\"Top 3 Accuracy\", ax=ax[2])\n",
    "hist.loc[:, (slice(None), \"acc_top5\")].droplevel(level=1,axis=1).plot(title=\"Top 5 Accuracy\", ax=ax[3])\n",
    "f.suptitle(\"All Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 model per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Kmean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Data through one model per reaction cluster\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# rxn_diff_fp width -> ohe width\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_clusters \u001b[39m=\u001b[39m Kmean\u001b[39m.\u001b[39mpredict(rxn_diff_fp_train)\n\u001b[1;32m      5\u001b[0m val_clusters \u001b[39m=\u001b[39m Kmean\u001b[39m.\u001b[39mpredict(rxn_diff_fp_val)\n\u001b[1;32m      7\u001b[0m cluster_models \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Kmean' is not defined"
     ]
    }
   ],
   "source": [
    "# Data through one model per reaction cluster\n",
    "# rxn_diff_fp width -> ohe width\n",
    "\n",
    "train_clusters = Kmean.predict(rxn_diff_fp_train)\n",
    "val_clusters = Kmean.predict(rxn_diff_fp_val)\n",
    "\n",
    "cluster_models = {}\n",
    "cluster_histories = {}\n",
    "\n",
    "for cluster in np.unique(train_clusters):\n",
    "    print(cluster)\n",
    "    x_train = torch.Tensor(rxn_diff_fp_train[train_clusters == cluster])\n",
    "    y_train = torch.Tensor(reag1_ohe_train[train_clusters == cluster])\n",
    "    x_val = torch.Tensor(rxn_diff_fp_val[val_clusters == cluster])\n",
    "    y_val = torch.Tensor(reag1_ohe_val[val_clusters == cluster])\n",
    "    fcrm = FullyConnectedReactionModel(\n",
    "        input_dim=x_train.shape[1],\n",
    "        hidden_dims=[600,600],\n",
    "        target_dim=y_train.shape[1],\n",
    "        hidden_act=torch.nn.ReLU, \n",
    "        output_act=torch.nn.Identity, \n",
    "        use_batchnorm=True, \n",
    "        dropout_prob=0.2,\n",
    "    )\n",
    "\n",
    "    optimizer=torch.optim.Adam(fcrm.parameters(), lr=lr)\n",
    "    scheduler = None#torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5, verbose=True)\n",
    "    early_stopper = None#EarlyStopper(patience=50, min_delta=0, verbose=True)\n",
    "\n",
    "    hist = train_loop(fcrm, x_train, y_train, epochs=e, batch_size=batch_size, loss_fn=torch.nn.CrossEntropyLoss(), optimizer=optimizer, report_freq=1, x_val=x_val, y_val=y_val, scheduler=scheduler, early_stopper=early_stopper)\n",
    "    cluster_models[cluster] = fcrm\n",
    "    cluster_histories[cluster] = hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.Tensor(reag1_ohe_train)\n",
    "y_val = torch.Tensor(reag1_ohe_val)\n",
    "pred_train = torch.zeros_like(y_train)\n",
    "pred_val = torch.zeros_like(y_val)\n",
    "for cluster in np.unique(train_clusters):\n",
    "    x_train = torch.Tensor(rxn_diff_fp_train[train_clusters == cluster])\n",
    "    x_val = torch.Tensor(rxn_diff_fp_val[val_clusters == cluster])\n",
    "    cluster_pred_train = cluster_models[cluster](x_train, training=False)\n",
    "    cluster_pred_val = cluster_models[cluster](x_val, training=False)\n",
    "    pred_train[train_clusters == cluster] = cluster_pred_train\n",
    "    pred_val[val_clusters == cluster] = cluster_pred_val\n",
    "\n",
    "acc_metric_top1 = torchmetrics.Accuracy(task=\"multiclass\", num_classes=y_train.shape[1], top_k=1)\n",
    "acc_metric_top3 = torchmetrics.Accuracy(task=\"multiclass\", num_classes=y_train.shape[1], top_k=3)\n",
    "acc_metric_top5 = torchmetrics.Accuracy(task=\"multiclass\", num_classes=y_train.shape[1], top_k=5)\n",
    "\n",
    "train_acc, train_acc_top3, train_acc_top5 = acc_metric_top1(pred_train, y_train.argmax(axis=1)).item(), acc_metric_top3(pred_train, y_train.argmax(axis=1)).item(), acc_metric_top5(pred_train, y_train.argmax(axis=1)).item()\n",
    "val_acc, val_acc_top3, val_acc_top5 = acc_metric_top1(pred_val, y_val.argmax(axis=1)).item(), acc_metric_top3(pred_val, y_val.argmax(axis=1)).item(), acc_metric_top5(pred_val, y_val.argmax(axis=1)).item()\n",
    "\n",
    "print(f\"Train      [ Acc (Top 1): {train_acc:.5f} | Acc (Top 3): {train_acc_top3:.5f} | Acc (Top 5): {train_acc_top5: .5f}]\")\n",
    "print(f\"Validation [ Acc (Top 1): {val_acc:.5f} | Acc (Top 3): {val_acc_top3:.5f} | Acc (Top 5): {val_acc_top5: .5f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemistry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1a6c9805cce96d1e29a306d4faa1abec69cf2c9ee1771799b926a7cc969fbbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
